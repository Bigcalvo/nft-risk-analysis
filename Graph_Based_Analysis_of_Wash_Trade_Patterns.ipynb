{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wash trade detection\n",
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def create_connection(df_file):\n",
    "    '''create connection to sqlite db\n",
    "    :param db_file: database file\n",
    "    :return: connection object\n",
    "    '''\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(df_file)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "conn = create_connection('nfts.sqlite')\n",
    "cursor = conn.cursor()\n",
    "query = '''\n",
    "SELECT * FROM transfers\n",
    "'''\n",
    "transfers_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# remove mints from transaction list (not applicable to wash trade detection) \n",
    "transfers_clean = transfers_df[transfers_df['to_address'] != '0x0000000000000000000000000000000000000000']\n",
    "\n",
    "transfers_clean = transfers_clean.reindex()\n",
    "\n",
    "# get list of all nodes, prepare for mapping\n",
    "nodes_1 = transfers_clean['from_address'].to_numpy()\n",
    "nodes_2 = transfers_clean['to_address'].to_numpy()\n",
    "nodes = np.append(nodes_1, nodes_2)\n",
    "nodes = set(nodes)\n",
    "\n",
    "# map original node IDs to numeric for faster computation\n",
    "nodes_map = {node:indx for indx, node in enumerate(nodes)}\n",
    "\n",
    "# same as above for token IDs\n",
    "token_ids = transfers_clean['token_id'].unique() # list of unique token IDs\n",
    "tokens_map = {token:indx for indx, token in enumerate(token_ids)}\n",
    "\n",
    "# convert to numpy array for faster computation\n",
    "transfers_array = transfers_clean.to_numpy() \n",
    "# 0:event_id, 1:trans_hash, 2:block_number, 3:nft_add, 4:token_id, 5:from, 6:to, 7:transaction_val, 8:timestamp\n",
    "\n",
    "# map node_id, token_id strings to numbers using our dict, should just be cleaner\n",
    "for i in transfers_array: \n",
    "    i[5] = nodes_map[i[5]]\n",
    "    i[6] = nodes_map[i[6]]\n",
    "    i[4] = tokens_map[i[4]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph generation for each collection and token, along with strong component detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating graph for each collection and token pairing\n",
    "token_graphs = {}\n",
    "for i in transfers_array:\n",
    "    if (i[3], i[4]) not in token_graphs: #<-- this is where we would add logic to include the address, and token id\n",
    "        G = nx.MultiDiGraph()\n",
    "        G.add_weighted_edges_from([(i[5], i[6], i[7])])\n",
    "        token_graphs[(i[3], i[4])] = {'graph': G}\n",
    "    else:\n",
    "        token_graphs[(i[3], i[4])]['graph'].add_weighted_edges_from([(i[5], i[6], i[7])])\n",
    "\n",
    "# for each graph, running strong component algorithm and tallying risky transactions/tokens\n",
    "for i in token_graphs:\n",
    "    all_components = nx.strongly_connected_components(token_graphs[i]['graph'])\n",
    "    token_graphs[i]['strong components'] = [i for i in list(all_components) if len(i) == 2]\n",
    "\n",
    "    # tally risky transactions for each token\n",
    "    if token_graphs[i]['strong components'] == []: # if no strong components\n",
    "        token_graphs[i]['risky trans'] = 0\n",
    "        token_graphs[i]['risky tokens'] = 0\n",
    "    else:\n",
    "        for edge in token_graphs[i]['strong components']:\n",
    "            src, targ = tuple(edge)\n",
    "            all_edge_data = [token_graphs[i]['graph'].get_edge_data(src, targ), token_graphs[i]['graph'].get_edge_data(targ, src)]\n",
    "            \n",
    "            if 'risky trans' not in token_graphs[i]:\n",
    "                token_graphs[i]['risky trans'] = sum(len(k) for k in all_edge_data)\n",
    "            else:\n",
    "                token_graphs[i]['risky trans'] += sum(len(k) for k in all_edge_data)\n",
    "            \n",
    "        token_graphs[i]['risky tokens'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDing self-trades and adding to risky token/transaction counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_transfers_array = transfers_array[transfers_array[:,5] == transfers_array[:,6]]\n",
    "\n",
    "# tallying self transfers/transactions as wash trading, adding to our risky trans count for each token\n",
    "for i in self_transfers_array:\n",
    "    if 'risky trans' not in token_graphs[(i[3], i[4])]:\n",
    "        token_graphs[(i[3], i[4])]['risky trans'] = 1\n",
    "    else:\n",
    "        token_graphs[(i[3], i[4])]['risky trans'] += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating risky business at the collection level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_collection_dict = {i:{'risky trans': token_graphs[i]['risky trans'], 'risky tokens': token_graphs[i]['risky tokens']} for i in token_graphs}\n",
    "new_df = pd.DataFrame.from_dict(filtered_collection_dict, orient='index')\n",
    "risky_trans_grouped = new_df.groupby(level=0).sum('risky trans').reset_index().rename(columns={'index': 'nft_address'})\n",
    "\n",
    "# query DB to get total transactions, tokens by collection\n",
    "totals_query = '''\n",
    "SELECT transactions.nft_address, COUNT(*)  AS trans_counts, toke_table.token_counts\n",
    "FROM transfers transactions\n",
    "JOIN  (\n",
    "\tSELECT nft_address,  COUNT(DISTINCT token_id ) AS token_counts\n",
    "\tFROM transfers\n",
    "\tGROUP BY nft_address) toke_table ON toke_table.nft_address = transactions.nft_address\n",
    "GROUP BY transactions.nft_address\n",
    "'''\n",
    "totals_df = pd.read_sql_query(totals_query, conn)\n",
    "\n",
    "combined_df = pd.merge(totals_df.assign(x=totals_df.nft_address.astype(str)), \\\n",
    "    risky_trans_grouped.assign(x=risky_trans_grouped.nft_address.astype(str)), \\\n",
    "    how='left', on='x')\n",
    "\n",
    "# cast nulls as 0 after joining totals and risky tables\n",
    "combined_df['risky trans'].fillna(0, inplace=True)\n",
    "combined_df['risky tokens'].fillna(0, inplace=True)\n",
    "\n",
    "combined_df.drop(['x', 'nft_address_y'], axis=1, inplace=True)\n",
    "\n",
    "# calculate new columns\n",
    "combined_df['risky_trans_ratio'] = combined_df['risky trans'] / combined_df['trans_counts']\n",
    "combined_df['risky_token_ratio'] = combined_df['risky tokens'] / combined_df['token_counts']\n",
    "\n",
    "combined_df.rename(columns={'nft_address_x':'nft_address',\\\n",
    "    'trans_counts':'total_transactions',\\\n",
    "    'token_counts':'total_tokens',\n",
    "    'risky trans':'risky_transactions',\n",
    "    'risky tokens':'risky_tokens',\n",
    "    'risky_trans_ratio':'risky_transaction_ratio'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write wash trade results to csv\n",
    "#pd.DataFrame.to_csv(combined_df, 'data/collection_wash_trades.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
